Starting training ...
Starting Compression ...
Using TensorFlow backend.
Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "compressor.py", line 20, in <module>
    import keras
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/biLSTM.hdf5 -model_name biLSTM -output ../data/compressed/files_to_be_compressed/biLSTM.compressed -batch_size 1000"
	User time (seconds): 0.57
	System time (seconds): 0.07
	Percent of CPU this job got: 89%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.72
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 72792
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 20007
	Voluntary context switches: 794
	Involuntary context switches: 6
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "decompressor.py", line 20, in <module>
    import keras
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/biLSTM.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/biLSTM.hdf5 -model_name biLSTM -input_file_prefix ../data/compressed/files_to_be_compressed/biLSTM.compressed -batch_size 1000"
	User time (seconds): 0.54
	System time (seconds): 0.08
	Percent of CPU this job got: 91%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.68
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 72716
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 19815
	Voluntary context switches: 794
	Involuntary context switches: 5
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
Traceback (most recent call last):
  File "compressor.py", line 202, in <module>
    main()
  File "compressor.py", line 164, in main
    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)
  File "compressor.py", line 69, in predict_lstm
    model.load_weights(args.model_weights_file)
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/engine/network.py", line 1157, in load_weights
    with h5py.File(filepath, mode='r') as f:
  File "/apps/python/3.6.1/lib/python3.6/site-packages/h5py/_hl/files.py", line 271, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/apps/python/3.6.1/lib/python3.6/site-packages/h5py/_hl/files.py", line 101, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper (/tmp/pip-s_7obrrg-build/h5py/_objects.c:2840)
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper (/tmp/pip-s_7obrrg-build/h5py/_objects.c:2798)
  File "h5py/h5f.pyx", line 78, in h5py.h5f.open (/tmp/pip-s_7obrrg-build/h5py/h5f.c:2117)
OSError: Unable to open file (Unable to open file: name = '../data/trained_models/files_to_be_compressed/bilstm.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/biLSTM.hdf5 -model_name biLSTM -output ../data/compressed/files_to_be_compressed/biLSTM.compressed -batch_size 1000"
	User time (seconds): 8.24
	System time (seconds): 1.30
	Percent of CPU this job got: 96%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:09.92
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 1115272
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 385635
	Voluntary context switches: 7412
	Involuntary context switches: 110
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
Traceback (most recent call last):
  File "decompressor.py", line 184, in <module>
    main()
  File "decompressor.py", line 153, in main
    f = open(args.input_file_prefix+'.combined','rb')
FileNotFoundError: [Errno 2] No such file or directory: '../data/compressed/files_to_be_compressed/biLSTM.compressed.combined'
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/biLSTM.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/biLSTM.hdf5 -model_name biLSTM -input_file_prefix ../data/compressed/files_to_be_compressed/biLSTM.compressed -batch_size 1000"
	User time (seconds): 2.11
	System time (seconds): 0.25
	Percent of CPU this job got: 92%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.54
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 236940
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 63195
	Voluntary context switches: 3282
	Involuntary context switches: 84
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
