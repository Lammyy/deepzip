Starting training ...
Starting Compression ...
Using TensorFlow backend.
Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "compressor.py", line 20, in <module>
    import keras
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -output ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 0.55
	System time (seconds): 0.09
	Percent of CPU this job got: 91%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.70
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 72800
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 20087
	Voluntary context switches: 793
	Involuntary context switches: 4
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "decompressor.py", line 20, in <module>
    import keras
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -input_file_prefix ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 0.53
	System time (seconds): 0.11
	Percent of CPU this job got: 90%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.72
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 72700
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 19702
	Voluntary context switches: 795
	Involuntary context switches: 12
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
usage: compressor.py [-h] [-model MODEL_WEIGHTS_FILE] [-model_name MODEL_NAME]
                     [-batch_size BATCH_SIZE] [-data SEQUENCE_NPY_FILE]
                     [-data_params PARAMS_FILE] [-output OUTPUT_FILE_PREFIX]
compressor.py: error: argument -model_name: expected one argument
Command exited with non-zero status 2
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -output ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.02
	System time (seconds): 0.25
	Percent of CPU this job got: 93%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.44
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 232900
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 62193
	Voluntary context switches: 3350
	Involuntary context switches: 84
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 2
Using TensorFlow backend.
usage: decompressor.py [-h] [-model MODEL_WEIGHTS_FILE]
                       [-model_name MODEL_NAME] [-batch_size BATCH_SIZE]
                       [-output OUTPUT_FILE_NAME]
                       [-input_file_prefix INPUT_FILE_PREFIX]
decompressor.py: error: argument -model_name: expected one argument
Command exited with non-zero status 2
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -input_file_prefix ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.01
	System time (seconds): 0.26
	Percent of CPU this job got: 93%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.45
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 232316
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 62492
	Voluntary context switches: 3275
	Involuntary context switches: 85
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 2
Starting training ...
Starting Compression ...
Using TensorFlow backend.
usage: compressor.py [-h] [-model MODEL_WEIGHTS_FILE] [-model_name MODEL_NAME]
                     [-batch_size BATCH_SIZE] [-data SEQUENCE_NPY_FILE]
                     [-data_params PARAMS_FILE] [-output OUTPUT_FILE_PREFIX]
compressor.py: error: argument -model_name: expected one argument
Command exited with non-zero status 2
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -output ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.06
	System time (seconds): 0.28
	Percent of CPU this job got: 91%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.57
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 232636
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 62024
	Voluntary context switches: 3294
	Involuntary context switches: 81
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 2
Using TensorFlow backend.
usage: decompressor.py [-h] [-model MODEL_WEIGHTS_FILE]
                       [-model_name MODEL_NAME] [-batch_size BATCH_SIZE]
                       [-output OUTPUT_FILE_NAME]
                       [-input_file_prefix INPUT_FILE_PREFIX]
decompressor.py: error: argument -model_name: expected one argument
Command exited with non-zero status 2
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -input_file_prefix ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.07
	System time (seconds): 0.27
	Percent of CPU this job got: 91%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.57
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 232324
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 62492
	Voluntary context switches: 3276
	Involuntary context switches: 89
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 2
Starting training ...
Starting Compression ...
Using TensorFlow backend.
usage: compressor.py [-h] [-model MODEL_WEIGHTS_FILE] [-model_name MODEL_NAME]
                     [-batch_size BATCH_SIZE] [-data SEQUENCE_NPY_FILE]
                     [-data_params PARAMS_FILE] [-output OUTPUT_FILE_PREFIX]
compressor.py: error: argument -model_name: expected one argument
Command exited with non-zero status 2
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -output ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.09
	System time (seconds): 0.22
	Percent of CPU this job got: 93%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.47
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 233168
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 62241
	Voluntary context switches: 3294
	Involuntary context switches: 84
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 2
Using TensorFlow backend.
usage: decompressor.py [-h] [-model MODEL_WEIGHTS_FILE]
                       [-model_name MODEL_NAME] [-batch_size BATCH_SIZE]
                       [-output OUTPUT_FILE_NAME]
                       [-input_file_prefix INPUT_FILE_PREFIX]
decompressor.py: error: argument -model_name: expected one argument
Command exited with non-zero status 2
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -input_file_prefix ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.01
	System time (seconds): 0.32
	Percent of CPU this job got: 93%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.49
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 232316
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 62472
	Voluntary context switches: 3277
	Involuntary context switches: 83
	Swaps: 0
	File system inputs: 0
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 2
Starting training ...
Starting Compression ...
Using TensorFlow backend.
Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "compressor.py", line 20, in <module>
    import keras
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -output ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 0.57
	System time (seconds): 0.05
	Percent of CPU this job got: 87%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.73
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 72848
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 19757
	Voluntary context switches: 791
	Involuntary context switches: 12
	Swaps: 0
	File system inputs: 24
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "decompressor.py", line 20, in <module>
    import keras
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/__init__.py", line 3, in <module>
    from . import utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/__init__.py", line 6, in <module>
    from . import conv_utils
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/utils/conv_utils.py", line 9, in <module>
    from .. import backend as K
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/__init__.py", line 89, in <module>
    from .tensorflow_backend import *
  File "/apps/keras/2.2.4-py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 5, in <module>
    import tensorflow as tf
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/__init__.py", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/apps/tensorflow/1.8.0-py36-gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 242, in load_module
    return load_dynamic(name, filename, file)
  File "/apps/python/3.6.1/lib/python3.6/imp.py", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Command exited with non-zero status 1
	Command being timed: "python decompressor.py -output ../data/compressed/files_to_be_compressed/.reconstructed.txt -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -input_file_prefix ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 0.59
	System time (seconds): 0.05
	Percent of CPU this job got: 85%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.75
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 72940
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 19674
	Voluntary context switches: 790
	Involuntary context switches: 15
	Swaps: 0
	File system inputs: 24
	File system outputs: 8
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Starting training ...
Starting Compression ...
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
Traceback (most recent call last):
  File "compressor.py", line 24, in <module>
    from keras.layers import LSTM, Flatten, CuDNNLSTM
ImportError: cannot import name CuDNNLSTM
Command exited with non-zero status 1
	Command being timed: "python compressor.py -data ../data/processed_files/files_to_be_compressed.npy -data_params ../data/processed_files/files_to_be_compressed.param.json -model ../data/trained_models/files_to_be_compressed/.hdf5 -model_name -output ../data/compressed/files_to_be_compressed/.compressed -batch_size 1000"
	User time (seconds): 2.17
	System time (seconds): 0.41
	Percent of CPU this job got: 89%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.90
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 206500
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 69262
	Voluntary context switches: 6108
	Involuntary context switches: 53
	Swaps: 0
	File system inputs: 0
	File system outputs: 16
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
